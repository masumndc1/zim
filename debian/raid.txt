

... raid or redundant arrays of independent disks.
... 
... there are different level of raid 
... raid 0: in raid 0 data are placed in two or more disk by striping.
... as data are placed in multiple disk any single failure of disk will
... result in data loss also raid 0 arrays cannot be rebuilt. in that
... case we have to take backups constantly.

... raid 1: which mirrors data between two or more devices, which means
... data is copied in multiple disk. we have always our data back when
... a single disk is operational in the arrays.

... raid 5: this level of raid has some kind of funtionality of raid 0 
... and raid 1. raid 5 stripes that data to multiple disk and place
... parity information in one disk. parity information is kind of meta
... data which is information of the data. parity is used to reconsturction
... of the data. in raid 5 say we have 5disk of 100G of space then the
... total capacity will be 400G for data and 100G(one disk) for parity.

... raid 6: raid 6 operation is similar to raid 5 except raid 6 uses
... two disk for parity. writing and reconstruction of data is different
... than raid 5.

... raid 10: traditionally raid 10 is built upon two or more raid 1 mirror
... and then creating one raid 0 using those raid 1 components. thus
... raid 10 has also named as raid 1+0. because of the design, raid 10 needs
... at least 4 disks. raid 0 arrays of two raid 1 mirror. each raid 1 
... mirror arrays has two disks.

... linux mdadm's raid 10: is has the function similar to traditional raid 1+0 but
... it can also use odd number of disk. it can chunk data across multiple disk.  

... we are now going to configure raid on ubuntu 16.04. for this we will use
... mdadm utility. this is the tools in linux used to administer the linux 
... software raid.

... if there is any pre existing array present then we can remove them like below

root@ubuntu:~# cat /proc/mdstat 
Personalities : [linear] [multipath] [raid0] [raid1] [raid6] [raid5] [raid4] [raid10] 
md0 : active raid0 sdc[1] sdd[0]
      209584128 blocks super 1.2 512k chunks
unused devices: <none>
root@ubuntu:~# 
root@ubuntu:~# umount /dev/md0
root@ubuntu:~# mdadm --stop /dev/md0
root@ubuntu:~# mdadm --remove /dev/md0
 
... we have to also discover the disk used to create the raid 

root@ubuntu:~# lsblk -o NAME,SIZE,FSTYPE,TYPE,MOUNTPOINT
NAME          SIZE FSTYPE      TYPE MOUNTPOINT
sda            12G             disk 
└─sda1         12G LVM2_member part 
  ├─vg1-root   47G ext4        lvm  /
  └─vg1-swap  2.8G swap        lvm  [SWAP]
sdb            15G LVM2_member disk 
└─vg1-root     47G ext4        lvm  /
sdc            15G LVM2_member disk 
└─vg1-root     47G ext4        lvm  /
sdd            10G LVM2_member disk 
└─vg1-root     47G ext4        lvm  /
sr0          1024M             rom  
root@ubuntu:~# 

... if we could find something like following then we have to zero their
... superblock to reset them to normal.

sdc      100G linux_raid_member disk 
sdd      100G linux_raid_member disk 


root@ubuntu:~# mdadm --zero-superblock /dev/sdc
root@ubuntu:~# mdadm --zero-superblock /dev/sdd

... we have to also remove reference in /etc/fstab too. like below

root@ubuntu:~# cat /etc/fstab
... output cut ...
# /dev/md0 /mnt/md0 ext4 defaults,nofail,discard 0 0

... Also, comment out or remove the array definition from the /etc/mdadm/mdadm.conf file:
... and update the initramfs

root@ubuntu:~# cat /etc/mdadm/mdadm.conf
. . .
# ARRAY /dev/md0 metadata=1.2 name=mdadmwrite:0 UUID=7261fb9c:976d0d97:30bc63ce:85e76e91

root@ubuntu:~# update-initramfs -u

... now we are ready to reuse the array again if there is any pre existed array.

... 1. our first example is creating raid 0 array.
... as we know that raid 0 chunks the data into multiple disk therefore the backup
... is mandatory otherwise the single failure of disk will cause loss of data.

... we will need at least two disks to create raid 0 array.
... let identify two raw disks.


root@ubuntu:~# lsblk -o NAME,SIZE,FSTYPE,TYPE,MOUNTPOINT
NAME          SIZE FSTYPE      TYPE MOUNTPOINT
sda            12G             disk 
└─sda1         12G LVM2_member part 
  ├─vg1-root   47G ext4        lvm  /
  └─vg1-swap  2.8G swap        lvm  [SWAP]
sdb            15G LVM2_member disk 
└─vg1-root     47G ext4        lvm  /
sdc            15G LVM2_member disk 
└─vg1-root     47G ext4        lvm  /
sdd            10G LVM2_member disk 
└─vg1-root     47G ext4        lvm  /
sde            10G             disk 
sdf            10G             disk 
sr0          1024M             rom  
root@ubuntu:~# 

... we will use sde and sdf for our raid 0 example. these two disks
... are raw disk without the filesystem.

... creating the raid 0 array

root@ubuntu:~# mdadm --create --verbose /dev/md0 --level=0 --raid-devices=2 /dev/sde /dev/sdf
mdadm: chunk size defaults to 512K
mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md0 started.
root@ubuntu:~# 
root@ubuntu:~# cat /proc/mdstat
Personalities : [linear] [multipath] [raid0] [raid1] [raid6] [raid5] [raid4] [raid10] 
md0 : active raid0 sdf[1] sde[0]
      20955136 blocks super 1.2 512k chunks
      
unused devices: <none>
root@ubuntu:~# 

... now following command is showing that /dev/sde and /dev/sdf is member of md0 raid 
... array

root@ubuntu:~# lsblk -o NAME,SIZE,FSTYPE,TYPE,MOUNTPOINT
NAME          SIZE FSTYPE            TYPE  MOUNTPOINT
sda            12G                   disk  
└─sda1         12G LVM2_member       part  
  ├─vg1-root   47G ext4              lvm   /
  └─vg1-swap  2.8G swap              lvm   [SWAP]
sdb            15G LVM2_member       disk  
└─vg1-root     47G ext4              lvm   /
sdc            15G LVM2_member       disk  
└─vg1-root     47G ext4              lvm   /
sdd            10G LVM2_member       disk  
└─vg1-root     47G ext4              lvm   /
sde            10G linux_raid_member disk  
└─md0          20G                   raid0 
sdf            10G linux_raid_member disk  
└─md0          20G                   raid0 
sr0          1024M                   rom   
root@ubuntu:~# 

... creating filesystem on the array and mount it.

root@ubuntu:~# df -h
Filesystem            Size  Used Avail Use% Mounted on
udev                  221M     0  221M   0% /dev
tmpfs                  49M  2.0M   47M   4% /run
/dev/mapper/vg1-root   47G  1.5G   43G   4% /
tmpfs                 244M     0  244M   0% /dev/shm
tmpfs                 5.0M     0  5.0M   0% /run/lock
tmpfs                 244M     0  244M   0% /sys/fs/cgroup
tmpfs                  49M     0   49M   0% /run/user/0
root@ubuntu:~# 
root@ubuntu:~# mkfs.ext4 -F /dev/md0
mke2fs 1.43.4 (31-Jan-2017)
Creating filesystem with 5238784 4k blocks and 1310720 inodes
Filesystem UUID: 2f7ff7c1-6b08-4a35-95de-f6a8c212a583
Superblock backups stored on blocks: 
	32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 
	4096000

Allocating group tables: done                            
Writing inode tables: done                            
Creating journal (32768 blocks): done
Writing superblocks and filesystem accounting information: done   

root@ubuntu:~# mkdir -p /mnt/md0
root@ubuntu:~# mount /dev/md0 /mnt/md0

... we can also make entry into the /etc/fstab file using the uuid 
... now we can find that the newly created file system has been updated.

root@ubuntu:~# df -h
Filesystem            Size  Used Avail Use% Mounted on
udev                  221M     0  221M   0% /dev
tmpfs                  49M  2.0M   47M   4% /run
/dev/mapper/vg1-root   47G  1.5G   43G   4% /
tmpfs                 244M     0  244M   0% /dev/shm
tmpfs                 5.0M     0  5.0M   0% /run/lock
tmpfs                 244M     0  244M   0% /sys/fs/cgroup
tmpfs                  49M     0   49M   0% /run/user/0
/dev/md0               20G   45M   19G   1% /mnt/md0
root@ubuntu:~# df -h -x devtmpfs -x tmpfs
root@ubuntu:~# 
Filesystem            Size  Used Avail Use% Mounted on
/dev/mapper/vg1-root   47G  1.5G   43G   4% /
/dev/md0               20G   45M   19G   1% /mnt/md0
root@ubuntu:~# 

... now we have to save the array layout which we can do by following
... automatically. and then update the initramfs so that the filesystem
... will be available during the early boot process

root@ubuntu:~# mdadm --detail --scan | tee -a /etc/mdadm/mdadm.conf 
ARRAY /dev/md0 metadata=1.2 name=ubuntu:0 UUID=e22d927a:223fe65f:4d1f051d:6a41dbfc
root@ubuntu:~# 

root@ubuntu:~# update-initramfs -u
update-initramfs: Generating /boot/initrd.img-4.10.0-19-generic
root@ubuntu:~# 

... we can add entry in the /etc/fstab to add the array at the boot time.
... we will use the uuid in the /etc/fstab which will be found from the 
... blkid command.
         
root@ubuntu:~# blkid
/dev/sda1: UUID="DMpNXf-TJ8t-nMao-TxhM-2TsB-PFE0-UuxMsY" TYPE="LVM2_member" PARTUUID="4c3c390a-01"
/dev/sdb: UUID="U6fQMc-oTvj-Mke9-orkI-G5CG-w9kb-qCInzm" TYPE="LVM2_member"
/dev/sdc: UUID="Ymz6yS-5rqL-5FmT-qmle-yam8-wuo0-S2iLwe" TYPE="LVM2_member"
/dev/sdd: UUID="kFTZ4U-kT39-fNFR-eHlN-e1bS-dDj1-rzdGww" TYPE="LVM2_member"
/dev/mapper/vg1-root: UUID="6e5a9d98-38e4-4ecb-baa5-c789b8bbb51a" TYPE="ext4"
/dev/mapper/vg1-swap: UUID="e23d3fad-ee26-45f5-a15c-39757138d0e6" TYPE="swap"
/dev/sde: UUID="e22d927a-223f-e65f-4d1f-051d6a41dbfc" UUID_SUB="d05e9aad-d689-078b-767e-48937194cd87" LABEL="ubuntu:0" TYPE="linux_raid_member"
/dev/sdf: UUID="e22d927a-223f-e65f-4d1f-051d6a41dbfc" UUID_SUB="beb25e1b-7d6e-11ca-d8ce-2b322b974872" LABEL="ubuntu:0" TYPE="linux_raid_member"
/dev/md0: UUID="2f7ff7c1-6b08-4a35-95de-f6a8c212a583" TYPE="ext4"
root@ubuntu:~# 

... now entry in the /etc/fstab will be something like following.

root@ubuntu:~# cat /etc/fstab
... output cut ...
UUID="2f7ff7c1-6b08-4a35-95de-f6a8c212a583" /mnt/md0 ext4 defaults,nofail,discard 0 0
... output cut ...

... 2. creating raid 1:
... to create raid 1 mirroring we need to remove the previously created raid 0 first.
... procedure which has been described at the first.
... to create the raid 1 mirroring we need at least the two disks.

root@ubuntu:~# mdadm --create --verbose /dev/md0 --level=1 --raid-devices=2 /dev/sde /dev/sdf
mdadm: Note: this array has metadata at the start and
    may not be suitable as a boot device.  If you plan to
    store '/boot' on this device please ensure that
    your boot-loader understands md/v1.x metadata, or use
    --metadata=0.90
mdadm: size set to 10477568K
Continue creating array? 
Continue creating array? (y/n) y
mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md0 started.
root@ubuntu:~# 

... following command ensures that raid 1 has been created

root@ubuntu:~# cat /proc/mdstat
Personalities : [linear] [multipath] [raid0] [raid1] [raid6] [raid5] [raid4] [raid10] 
md0 : active raid1 sdf[1] sde[0]
      10477568 blocks super 1.2 [2/2] [UU]
      
unused devices: <none>
root@ubuntu:~# 

... rest of the procedure is the same as was with raid 0.

root@ubuntu:~# 
root@ubuntu:~# mkfs.ext4 -F /dev/md0
mke2fs 1.43.4 (31-Jan-2017)
/dev/md0 contains a ext4 file system
	last mounted on Fri Feb  2 01:28:51 2018
Creating filesystem with 2619392 4k blocks and 655360 inodes
Filesystem UUID: a2e55a7b-7ac0-451b-b7f4-2d97ec183f6a
Superblock backups stored on blocks: 
	32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632

Allocating group tables: done                            
Writing inode tables: done                            
Creating journal (16384 blocks): done
Writing superblocks and filesystem accounting information: done 

root@ubuntu:~# mount /dev/md0 /mnt/md0
root@ubuntu:~# 
root@ubuntu:~# df -h
Filesystem            Size  Used Avail Use% Mounted on
udev                  221M     0  221M   0% /dev
tmpfs                  49M  2.0M   47M   4% /run
/dev/mapper/vg1-root   47G  1.5G   43G   4% /
tmpfs                 244M     0  244M   0% /dev/shm
tmpfs                 5.0M     0  5.0M   0% /run/lock
tmpfs                 244M     0  244M   0% /sys/fs/cgroup
tmpfs                  49M     0   49M   0% /run/user/0
/dev/md0              9.8G   37M  9.3G   1% /mnt/md0
root@ubuntu:~# 
root@ubuntu:~# mdadm --detail --scan | tee -a /etc/mdadm/mdadm.conf
ARRAY /dev/md0 metadata=1.2 name=ubuntu:0 UUID=c50086bb:1bd49345:1961b677:2fe17d8f
root@ubuntu:~# 
root@ubuntu:~# update-initramfs -u
update-initramfs: Generating /boot/initrd.img-4.10.0-19-generic
root@ubuntu:~# 

... now we are on our next phase raid 5 and finally raid 10. 
... procedure is almost same, therefore only the command will be
... shown here.

... we will comment out array line in /etc/mdadm/mdadm.conf

root@ubuntu:~# cat /etc/mdadm/mdadm.conf 
... output cut ... 
#ARRAY /dev/md0 metadata=1.2 name=ubuntu:0 UUID=e22d927a:223fe65f:4d1f051d:6a41dbfc
#ARRAY /dev/md0 metadata=1.2 name=ubuntu:0 UUID=c50086bb:1bd49345:1961b677:2fe17d8f
root@ubuntu:~# 

... we will reset the raid 1 first by using the following script.

root@ubuntu:~# cat removing_raid_array.sh 
#!/bin/bash

umount /dev/md0
mdadm --stop /dev/md0
mdadm --remove /dev/md0

# zero-superblock
mdadm --zero-superblock /dev/sde
mdadm --zero-superblock /dev/sdf

# update the initramfs 
update-initramfs -u

root@ubuntu:~# 

root@ubuntu:~# bash +x removing_raid_array.sh 
mdadm: stopped /dev/md0
update-initramfs: Generating /boot/initrd.img-4.10.0-19-generic
W: mdadm: /etc/mdadm/mdadm.conf defines no arrays.
root@ubuntu:~# 

... again we will create raid 5 by using the following script.
... as raid 5 use one disk for the parity therefore we need 3disks
... two for data and one for the parity.

... to create raid 5 we will use the following script.

root@ubuntu:~# cat raid5.sh 
#!/bin/bash

echo -e " creating raid 5 array"
mdadm --create --verbose /dev/md0 --level=5 --raid-devices=3 /dev/sde /dev/sdf /dev/sdg

echo -e " checking if the array created "
cat /proc/mdstat

echo -e " creating filesystem "
mkfs.ext4 -F /dev/md0

echo -e " mounting the filesystem "
mount /dev/md0 /mnt/md0

echo -e " checking the new space created "
df -h -x devtmpfs -x tmpfs

echo -e " updating the mdadm.conf file"
mdadm --detail --scan | tee -a /etc/mdadm/mdadm.conf

echo -e " updating the initramfs "
update-initramfs -u

root@ubuntu:~# 

... snip of the output from the command raid5.sh 

 checking the new space created 
Filesystem            Size  Used Avail Use% Mounted on
/dev/mapper/vg1-root   47G  1.5G   43G   4% /
/dev/md0               20G   45M   19G   1% /mnt/md0
 updating the mdadm.conf file
ARRAY /dev/md0 metadata=1.2 name=ubuntu:0 UUID=4cd259f5:a20e2137:8a4be9bd:fc707ffa
 updating the initramfs 
update-initramfs: Generating /boot/initrd.img-4.10.0-19-generic
root@ubuntu:~# 

... the /dev/md0 is showing 20G total as we have take 3 disks of same 10G. 2*10G have
... been taken for data and rest of 10G has selected for parity.

... at last phase we will add the entry in /etc/fstab like below.
... uuid can be found from the command blkid


root@ubuntu:~# 
root@ubuntu:~# blkid | grep md0
/dev/md0: UUID="b66441f1-5ba4-4486-9f77-eb53b0bc7b27" TYPE="ext4"
root@ubuntu:~# 
root@ubuntu:~# cat /etc/fstab 
# /etc/fstab: static file system information.
#
# Use 'blkid' to print the universally unique identifier for a
# device; this may be used with UUID= as a more robust way to name devices
# that works even if disks are added and removed. See fstab(5).
#
# <file system> <mount point>   <type>  <options>       <dump>  <pass>
/dev/mapper/vg1-root /               ext4    errors=remount-ro 0       1
/dev/mapper/vg1-swap none            swap    sw              0       0

# entry for the /dev/md0 array 
UUID="b66441f1-5ba4-4486-9f77-eb53b0bc7b27" /mnt/md0 ext4 	default,nofail,discard 0	0
root@ubuntu:~# 

... creating the raid6 array 
... the procedure is exactly same but just one command of making the array

root@ubuntu:~# mdadm --create --verbose /dev/md0 --level=6 --raid-devices=4 /dev/sde /dev/sdf /dev/sdg /dev/sdh

... rest of the procedure is exactly same.

... creating raid 10. here we are not creating traditional raid 10 but mdadm's raid 10
... which requires odd number of disks too. traditional raid 10 requires at least 4 disks.
... the procedure is exactly same as raid 5 and raid6 but just one command of making the array
... in this example we will use the minimum 3disks

root@ubuntu:~# mdadm --create --verbose /dev/md0 --level=10 --raid-devices=4 /dev/sde /dev/sdf /dev/sdg
/dev/sdh

... rest of the procedure is exactly same.

... in mdadm's 10 raid there are three layout near(default), far and offset.
... we can choose the layout by using options --layout.
... the above command is making two copies of data which is default.
... when we dont specify any layout then it assume near and copies will be 2.

... we can provide layout and copies of the data.
... the following command is telling us the layout will be offset and 3 copies.

root@ubuntu:~# mdadm --create --verbose /dev/md0 --level=10 --layout=o3 --raid-devices=4 /dev/sda /dev/sdb /dev/sdc /dev/sdd


.... query on array:
... some related command on raid.


root@ubuntu:~# mdadm -D /dev/md0
/dev/md0:
        Version : 1.2
  Creation Time : Sun Feb  4 15:57:33 2018
     Raid Level : raid5
     Array Size : 20955136 (19.98 GiB 21.46 GB)
  Used Dev Size : 10477568 (9.99 GiB 10.73 GB)
   Raid Devices : 3
  Total Devices : 3
    Persistence : Superblock is persistent

    Update Time : Sun Feb  4 16:11:32 2018
          State : clean 
 Active Devices : 3
Working Devices : 3
 Failed Devices : 0
  Spare Devices : 0

         Layout : left-symmetric
     Chunk Size : 512K

           Name : ubuntu:0  (local to host ubuntu)
           UUID : 4cd259f5:a20e2137:8a4be9bd:fc707ffa
         Events : 22

    Number   Major   Minor   RaidDevice State
       0       8       64        0      active sync   /dev/sde
       1       8       80        1      active sync   /dev/sdf
       3       8       96        2      active sync   /dev/sdg
root@ubuntu:~# 

... brief info 

root@ubuntu:~#  mdadm -Db /dev/md0
ARRAY /dev/md0 metadata=1.2 name=ubuntu:0 UUID=4cd259f5:a20e2137:8a4be9bd:fc707ffa
root@ubuntu:~# 

root@ubuntu:~# mdadm -Q /dev/md0
/dev/md0: 19.98GiB raid5 3 devices, 0 spares. Use mdadm --detail for more detail.
root@ubuntu:~# 

... quering an disk

root@ubuntu:~# mdadm -Q /dev/sdf
/dev/sdf: is not an md array
/dev/sdf: device 1 in 3 device active raid5 /dev/md0.  Use mdadm --examine for more detail.
root@ubuntu:~# 
root@ubuntu:~# mdadm -E /dev/sdf
/dev/sdf:
          Magic : a92b4efc
        Version : 1.2
    Feature Map : 0x0
     Array UUID : 4cd259f5:a20e2137:8a4be9bd:fc707ffa
           Name : ubuntu:0  (local to host ubuntu)
  Creation Time : Sun Feb  4 15:57:33 2018
     Raid Level : raid5
   Raid Devices : 3

 Avail Dev Size : 20955136 (9.99 GiB 10.73 GB)
     Array Size : 20955136 (19.98 GiB 21.46 GB)
    Data Offset : 16384 sectors
   Super Offset : 8 sectors
   Unused Space : before=16296 sectors, after=0 sectors
          State : clean
    Device UUID : 03f6e6f5:146eaa34:c34bfdbf:abdd7b74

    Update Time : Sun Feb  4 16:11:32 2018
  Bad Block Log : 512 entries available at offset 72 sectors
       Checksum : 43053c78 - correct
         Events : 22

         Layout : left-symmetric
     Chunk Size : 512K

   Device Role : Active device 1
   Array State : AAA ('A' == active, '.' == missing, 'R' == replacing)
root@ubuntu:~# 
root@ubuntu:~# cat /proc/mdstat 
Personalities : [linear] [multipath] [raid0] [raid1] [raid6] [raid5] [raid4] [raid10] 
md0 : active raid5 sdg[3] sdf[1] sde[0]
      20955136 blocks super 1.2 level 5, 512k chunk, algorithm 2 [3/3] [UUU]
      
unused devices: <none>
root@ubuntu:~# 

... start the array. and specific devices

root@ubuntu:~# mdadm --assemble --scan
root@ubuntu:~# mdadm --assemble /dev/md0

... This works if the array is defined in the configuration file.
... If the correct definition for the array is missing from the 
... configuration file, the array can still be started by passing in 
... the component devices:

root@ubuntu:~# mdadm --assemble /dev/md0 /dev/sda /dev/sdb /dev/sdc /dev/sdd

... adding new raw spare disk to the array
... and update the /etc/mdadm/mdadm.conf file like below.

root@ubuntu:~# mdadm /dev/md0 --add /dev/sde
root@ubuntu:~# mdadm --detail --brief /dev/md0 | tee -a /etc/mdadm/mdadm.conf
root@ubuntu:~# 

... after that we check our added disk with array like below.

root@ubuntu:~# mdadm --detail /dev/md0
/dev/md0:
        Version : 1.2
  Creation Time : Sun Feb  4 15:57:33 2018
     Raid Level : raid5
     Array Size : 20955136 (19.98 GiB 21.46 GB)
  Used Dev Size : 10477568 (9.99 GiB 10.73 GB)
   Raid Devices : 3
  Total Devices : 3
    Persistence : Superblock is persistent

    Update Time : Sun Feb  4 16:11:32 2018
          State : clean 
 Active Devices : 3
Working Devices : 3
 Failed Devices : 0
  Spare Devices : 0

         Layout : left-symmetric
     Chunk Size : 512K

           Name : ubuntu:0  (local to host ubuntu)
           UUID : 4cd259f5:a20e2137:8a4be9bd:fc707ffa
         Events : 22

    Number   Major   Minor   RaidDevice State
       0       8       64        0      active sync   /dev/sde
       1       8       80        1      active sync   /dev/sdf
       3       8       96        2      active sync   /dev/sdg
root@ubuntu:~# 
root@ubuntu:~# 
root@ubuntu:~# cat /proc/mdstat 
Personalities : [linear] [multipath] [raid0] [raid1] [raid6] [raid5] [raid4] [raid10] 
md0 : active raid5 sdg[3] sdf[1] sde[0]
      20955136 blocks super 1.2 level 5, 512k chunk, algorithm 2 [3/3] [UUU]
      
unused devices: <none>
root@ubuntu:~# 

... we may need to use following command 

root@ubuntu:~# mdadm --grow /dev/md0 --raid-devices=3 --add /dev/sdc
root@ubuntu:~# resize2fs /dev/md0

... removing the device

root@ubuntu:~# mdadm /dev/md0 --fail /dev/sdc
root@ubuntu:~# mdadm /dev/md0 --remove /dev/sdc

... then we can add new disk to the array like following.

root@ubuntu:~# mdadm /dev/md0 --add /dev/sdd
















